1. - Difference between collections and stream?
| Feature          | **Collections**                                                                 | **Streams**                                                                                                  |
| ---------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Definition**   | Data structure that stores **elements in memory** (e.g., `List`, `Set`, `Map`). | A **pipeline of data** (sequence of elements) supporting functional-style operations.                        |
| **Nature**       | **Eager** ‚Äì you store and access elements directly.                             | **Lazy** ‚Äì operations are executed only when a terminal operation (like `collect()`, `forEach()`) is called. |
| **Storage**      | Actually **stores data** (in memory).                                           | Does **not store data**; it computes results from a source (collection, array, I/O channel).                 |
| **Traversal**    | Can be traversed **multiple times**.                                            | A stream can be traversed **only once** (after a terminal operation, it‚Äôs consumed).                         |
| **Modification** | You can **add/remove/update** elements.                                         | Streams are **read-only** (cannot modify the underlying data).                                               |
| **Iteration**    | External iteration (e.g., `for-each` loop).                                     | Internal iteration (handled by stream API).                                                                  |
| **Parallelism**  | Manual (`for` loops, `ExecutorService`).                                        | Built-in (`parallelStream()`).                                                                               |
| **Operations**   | Focused on **data storage & structure manipulation**.                           | Focused on **data processing (map, filter, reduce, collect)**.                                               |

‚úÖ Interview-Ready Answer
Collections are about storing and managing data in memory, while Streams are about processing data in a functional, declarative way.
Collections use external iteration and allow modifications, while Streams use internal iteration, are lazy, can be parallelized easily, and are consumed only once.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. - Internal Working of HashMap, HashSet, TreeMap and TreeSet and ConcurrentHashMap?

üîπ 1. HashMap (Key‚ÄìValue Pair Storage)
Data Structure: Uses array of buckets internally (each bucket is a linked list / tree in Java 8+).
How it works:
When you put a key-value pair (put(key, value)), the hashCode() of the key is computed.
The hash is converted to an index (bucket position).
If the bucket is empty ‚Üí insert directly.
If not empty ‚Üí compare using equals() to avoid duplicates.

Before Java 8: bucket is a LinkedList.
Java 8+: If collisions become large (>8 elements), LinkedList is converted to a balanced tree (Red-Black Tree).
On get(key) ‚Üí same hash process to find the bucket ‚Üí scan list/tree for match using equals().
Key Points:
Allows 1 null key, multiple null values.
Order is not guaranteed.
Time complexity: O(1) avg, O(log n) in case of tree bucket.

üîπ 2. HashSet
Built on top of HashMap.
Working:
When you do set.add(x), internally it calls map.put(x, PRESENT), where PRESENT is a dummy constant object.
Duplicate check is handled by HashMap‚Äôs containsKey().
Key Points:
Stores only unique elements.
Allows 1 null element.
Order is not guaranteed.

üîπ 3. TreeMap (Sorted Key‚ÄìValue Map)
Data Structure: Red-Black Tree (Self-balancing BST).
How it works:
Keys are sorted according to their natural order (via Comparable) or using a Comparator.
Insertions and lookups walk down the tree and maintain balance (via color-flipping & rotations).
Key Points:
Does not allow null key (throws NullPointerException), but values can be null.
Order is sorted (ascending by default).
Time complexity: O(log n) for put/get.

üîπ 4. TreeSet
Built on top of TreeMap.
Working:
When you do set.add(x), internally it calls map.put(x, PRESENT).
Sorting order maintained via TreeMap‚Äôs Red-Black Tree.
Key Points:
Stores unique elements in sorted order.
Does not allow null element (from Java 7 onwards if using natural ordering).
Time complexity: O(log n) for add, remove, contains.

üîπ 5. ConcurrentHashMap
Thread-safe version of HashMap.
How it works (Java 8):
Data is stored in an array of Nodes (similar to HashMap).
Instead of locking the entire map, it uses fine-grained locking:
Before Java 8: Segmented locking (each segment = smaller HashMap with a lock).
Java 8+: Uses CAS (Compare-And-Swap) + synchronized blocks on buckets only when needed.
Read operations (get) are non-blocking (almost O(1)).
Write operations (put, remove) lock only the affected bucket.
Key Points:
No null key or null value allowed.
Faster than Hashtable and Collections.synchronizedMap().
Suitable for concurrent environments.

üñºÔ∏è Quick Analogy

HashMap ‚Üí A big cupboard with labeled boxes (fast lookup, unordered).
HashSet ‚Üí Just the labels of the cupboard, no values inside.
TreeMap ‚Üí Cupboard where keys are always arranged in sorted order.
TreeSet ‚Üí Just the sorted labels of the cupboard.
ConcurrentHashMap ‚Üí A cupboard with multiple workers,each worker locks only a single box instead of locking the whole cupboard.

‚ö° Interview Tip:
If asked "Why does ConcurrentHashMap not allow null key/value?" ‚Üí
üëâ Because in multi-threaded environments, a null return from get(key) would create ambiguity:
Did the mapping not exist?
Or was the value actually null?

why treemap doesnt allow null keys?
‚úÖ Why TreeMap doesn‚Äôt allow null keys
TreeMap uses Red-Black Tree internally
Keys are stored in sorted order.
Sorting is done either by:
Natural ordering (Comparable), or
A Comparator you provide.

Comparison Problem
When inserting a new key, TreeMap must compare it with existing keys (compareTo() or compare()).
If the key is null ‚Üí calling null.compareTo(something) causes NullPointerException.
Example:
TreeMap<String, Integer> map = new TreeMap<>();
map.put(null, 1);  // NPE at runtime
Design Choice (Clarity)
HashMap can allow one null key because it doesn‚Äôt sort ‚Äî it just hashes it.
TreeMap must always maintain order, so null key would break the comparison logic.
To keep things consistent and predictable, Java designers decided: no null keys in TreeMap.
‚úÖ Why null values are allowed in TreeMap
TreeMap compares keys, not values.
So values can be null safely.
üéØ Interview-ready Answer
TreeMap does not allow null keys because it sorts keys using compareTo() or a Comparator, and comparing null with other keys would throw NullPointerException.
On the other hand, HashMap allows one null key since hashing does not require comparisons.

| Collection Type        | Null Key Allowed?                    | Null Value Allowed?          | Notes                                                                      |
| ---------------------- | ------------------------------------ | ---------------------------- | -------------------------------------------------------------------------- |
| **HashMap**            | ‚úÖ Yes (only **1** null key)          | ‚úÖ Yes (multiple null values) | Stores key-value pairs in buckets (hashing).                               |
| **HashSet**            | ‚úÖ Yes (only **1** null element)      | N/A                          | Built on top of `HashMap`, so 1 null element allowed.                      |
| **TreeMap**            | ‚ùå No (throws `NullPointerException`) | ‚úÖ Yes                        | Keys must be comparable ‚Üí `null` breaks comparison.                        |
| **TreeSet**            | ‚ùå No (throws `NullPointerException`) | N/A                          | Built on top of `TreeMap`, so no null elements.                            |
| **ConcurrentHashMap**  | ‚ùå No                                 | ‚ùå No                         | Disallows null key & values to avoid ambiguity in concurrent environments. |
| **Hashtable** (legacy) | ‚ùå No                                 | ‚ùå No                         | Thread-safe, but stricter rules than `HashMap`.                            |

--------------------------------------------------------------------------------------------

what does it mean by concurrent environment?
A concurrent environment is when multiple threads are executing at the same time and may
try to access or modify shared resources (like variables, objects, or collections).

In single-threaded code ‚Üí only one thread runs at a time, so no conflicts.

In multi-threaded / concurrent code ‚Üí two or more threads may read/write the same data simultaneously,
which can lead to race conditions, data corruption, or inconsistent state.
One thread might be writing to the bucket, another thread reading from the same bucket.
This can lead to inconsistent state, lost updates, or infinite loops (in HashMap before Java 8).

‚úÖ Why ConcurrentHashMap is needed
A normal HashMap is not thread-safe ‚Üí fails in concurrent environments.
ConcurrentHashMap is designed for concurrency:
Multiple threads can safely read without blocking.
Write operations lock only the specific bucket (not the whole map).
Prevents corruption in concurrent scenarios.

-------------------------------------------------------------------------------------

How ConcurrentHashMap avoids locking the entire map (i.e., bucket-level locking with CAS) in a visual way for interviews?
üîπ How ConcurrentHashMap avoids locking the entire map
‚úÖ Old way (Hashtable / SynchronizedMap)
Every read/write was wrapped with a synchronized block.
So if 1 thread was updating a value, all other threads were blocked ‚Äî even for simple reads.
This caused huge performance bottlenecks in concurrent environments.

‚úÖ New way (ConcurrentHashMap in Java 8+)
Does not lock the whole map.
Uses fine-grained locking + CAS (Compare-And-Swap)
üî∏ Internal Structure
Data stored as Node[] table (like HashMap).
Each bucket (index in the table) can have:
A Node (key, value, next)
Or a TreeNode (red-black tree) when collisions are high

üî∏ Operations
Reads (get)
Completely lock-free.
Just compute hash, go to bucket, and traverse linked list/tree.
Uses volatile fields so threads see the latest values.
‚úÖ Multiple threads can read simultaneously ‚Üí very fast.

Writes (put, remove)
Only lock the specific bucket (bin) being updated.
Done using synchronized on Node level (not entire map).
Steps:
Compute hash ‚Üí find bucket index.
If bucket empty ‚Üí try to insert using CAS (no locking needed).
If bucket occupied ‚Üí synchronize only that bucket ‚Üí insert/update.
‚úÖ Other buckets remain free for other threads.
Resizing (rehashing)
Even resizing is parallelized.
Multiple threads help redistribute buckets into the new table.

üñºÔ∏è Visual Analogy
Imagine a cupboard with many drawers (buckets):
Hashtable ‚Üí Only 1 key üóùÔ∏è for the entire cupboard ‚Üí if one person is using it, others must wait.
ConcurrentHashMap ‚Üí Every drawer has its own small lock üîí ‚Üí multiple people can use different drawers at the same time.

üéØ Interview-ready Summary
ConcurrentHashMap achieves thread safety without locking the entire map by using bucket-level locking and CAS operations.
Reads are completely lock-free.
Writes only lock the specific bucket being updated.
This allows multiple threads to work in parallel, making it much faster than Hashtable or Collections.synchronizedMap() in concurrent environments.

‚ö° Bonus: If asked ‚ÄúWhy CAS?‚Äù ‚Üí
üëâ CAS (Compare-And-Swap) is a non-blocking atomic operation that updates a value only if it has not changed by another thread.
This avoids heavy locking and improves concurrency.
üéØ How this connects to ConcurrentHashMap CHM
Instead of locking entire structure, CHM uses CAS for some operations (like inserting in an empty bucket).
If CAS succeeds ‚Üí update happens without locking.
If CAS fails ‚Üí falls back to bucket-level synchronization.
So CAS is the first line of defense, and fine-grained locking is the backup.
üîπ What CAS actually does
Imagine you have a box that stores a number.
You think the number is X.
You want to change it to Y.
But before you do, you check: ‚ÄúIs the box still X?‚Äù
If yes ‚Üí update it to Y. ‚úÖ
If no ‚Üí do nothing. ‚ùå
That is Compare-And-Swap
It is an atomic CPU instruction that ensures only one thread‚Äôs update succeeds when multiple threads try at the same time.

AtomicInteger count = new AtomicInteger(0);
// Thread 1
count.compareAndSet(0, 1); // "If value is 0, set to 1"
Here‚Äôs what happens internally:
Thread 1 sees value = 0.
It compares 0 with expected 0. ‚úÖ Match!
Swaps value ‚Üí 1.

// Thread 2
count.compareAndSet(0, 2); // "If value is 0, set to 2"
Thread 2 thinks value is still 0.
But after Thread 1, the value is actually 1. ‚ùå
Compare fails ‚Üí Thread 2‚Äôs update is rejected.
So only one thread wins, without using a lock.

üîπ Why is this useful?
In a concurrent environment.
Many threads may want to update the same variable.
Instead of blocking each other with synchronized, CAS lets threads try updates in parallel.
Only the thread that finds the ‚Äúexpected value‚Äù succeeds.

üî∏ Visual Analogy
Think of CAS like putting your name on a form:
You say: ‚ÄúIf the form is still blank, I‚Äôll write my name.‚Äù
If someone else already wrote their name, your attempt fails.
No fighting, no waiting ‚Äî just check and update in one step.

üëâ So in short: CAS = check current value ‚Üí if it matches what I expect, update it atomically. Else, fail.

Example:- RaceConditionCASFix.java

---------------------------------------------------------------------------------------------------------

üîπ What is ConcurrentModificationException?
It‚Äôs a runtime exception that occurs when a collection (like ArrayList, HashMap, etc.) is structurally modified while being iterated,
using an iterator or enhanced-for loop.
Structural modification ‚Üí adding/removing elements that change the size of the collection.
Safe modification ‚Üí only through the iterator‚Äôs own remove() method.

üîπ Why does this happen?
Most collection classes (like ArrayList, HashMap) are fail-fast.
When you create an iterator, it keeps an internal modCount (modification count).
Every structural modification updates modCount
If during iteration, iterator detects modCount has changed unexpectedly ‚Üí it throws ConcurrentModificationException.

üîπ How to avoid CME?
‚úÖ Use iterator‚Äôs remove() method
‚úÖ Use CopyOnWriteArrayList (thread-safe, avoids CME)
üîπ In Multi-threading Context
CME also occurs if:
One thread is iterating a collection,
Another thread modifies it at the same time.
That‚Äôs why we use ConcurrentHashMap, CopyOnWriteArrayList, etc. in concurrent environments.

‚úÖ Interview One-Liner:
ConcurrentModificationException happens because most collections are fail-fast ‚Äî
if the collection is structurally modified outside the iterator, iteration is no longer safe, so Java throws this exception to prevent unpredictable behavior.

üîπ Why does list.remove(s) sometimes work without exception?
The enhanced-for loop is syntax sugar for using an Iterator.
for (String s : list) { ... }
becomes
Iterator<String> it = list.iterator();
while (it.hasNext()) {
    String s = it.next();
    ...
}
When you call list.remove(s) directly, it modifies the list but not the iterator‚Äôs internal state.
The next call to it.next() checks the modCount mismatch ‚Üí only then CME is thrown.

üîπ Cases
Case 1: Single removal, then loop ends
If you remove the last element being iterated (or remove once and iteration finishes before it.next() is called again), you might not see CME.
Case 2: More iterations after removal.
If iteration continues, the next it.next() will detect modCount mismatch ‚Üí CME.

‚úÖ One-liner for interview:
list.remove(s) inside a loop may or may not throw ConcurrentModificationException immediately ‚Äî
the exception is thrown only when the iterator detects a modCount mismatch on the next iteration step.

-----------------------------------------------------------------------------------------

- Which Data structure to use for frequent data update, remove data and add new data & searching?
üîπ Requirements
Frequent updates
Frequent additions
Frequent removals
Efficient searching
So we want fast insert, delete, and search.

üîπ Data Structure Options
1. Array / ArrayList
‚úÖ Fast random access (O(1) by index).
‚ùå Insertion/deletion in the middle is costly (O(n)) because of shifting.
‚ùå Not ideal for frequent add/remove.

2. LinkedList
‚úÖ Fast insertion/deletion (O(1) if node reference known).
‚ùå Searching is slow (O(n)).
‚ùå Not good if search is frequent.

3. HashMap / HashSet
‚úÖ Average O(1) for insert, delete, update, search by key.
‚úÖ Best when search is based on a key (hashable).
‚ùå No ordering of elements.
‚ùå Worst-case O(n) if hash collisions happen (though rare with good hash).
üëâ Best choice when you need frequent add/remove/search with keys.

4. TreeMap / TreeSet (Balanced BST like Red-Black Tree)
‚úÖ O(log n) for insert, delete, search.
‚úÖ Maintains sorted order.
‚ùå Slower than HashMap for pure key lookups.
üëâ Best choice when you need sorted data + frequent updates.

5. ConcurrentHashMap
‚úÖ Like HashMap but thread-safe.
‚úÖ Ideal in concurrent environments.

6. Hybrid Approach
Sometimes we combine:
HashMap + LinkedList ‚Üí LRU Cache (fast lookup + ordered eviction).
HashMap + Tree ‚Üí fast lookups + ordered iteration.

üîπ Final Answer (for interviews)
If you need fastest add/remove/update/search ‚Üí HashMap / HashSet (average O(1)).
If you also need sorted order ‚Üí TreeMap / TreeSet (O(log n)).
If operations happen in multi-threaded environment ‚Üí ConcurrentHashMap.

‚úÖ One-liner to say in interview:
‚ÄúFor frequent insert, delete, update, and search,
a HashMap (or ConcurrentHashMap in multi-threaded context) is best because it provides average O(1) performance for these operations.
If ordering is required, then a TreeMap is more suitable, with O(log n) operations.‚Äù

üîπ Comparison of Common Data Structures in Java

| Data Structure        | Search (by value)        | Insert                            | Delete                            | Update          | Ordered?            | Notes                                                      |
| --------------------- | ------------------------ | --------------------------------- | --------------------------------- | --------------- | ------------------- | ---------------------------------------------------------- |
| **Array / ArrayList** | `O(n)` (linear search)   | `O(1)` at end, `O(n)` in middle   | `O(n)` (shift needed)             | `O(1)` by index | ‚úÖ (insertion order) | Great for random access, bad for frequent updates/removals |
| **LinkedList**        | `O(n)`                   | `O(1)` if node known, else `O(n)` | `O(1)` if node known, else `O(n)` | `O(n)`          | ‚úÖ (insertion order) | Good for sequential access, poor for search                |
| **HashMap / HashSet** | `O(1)` avg, `O(n)` worst | `O(1)` avg                        | `O(1)` avg                        | `O(1)` avg      | ‚ùå (unordered)       | Best for frequent add/remove/search by key                 |
| **TreeMap / TreeSet** | `O(log n)`               | `O(log n)`                        | `O(log n)`                        | `O(log n)`      | ‚úÖ (sorted order)    | Good balance when you need sorted + efficient ops          |
| **ConcurrentHashMap** | `O(1)` avg               | `O(1)` avg                        | `O(1)` avg                        | `O(1)` avg      | ‚ùå                   | Thread-safe, used in concurrent environments               |

üîπ Interview-ready Answer
üëâ If interviewer asks ‚ÄúWhich DS for frequent add/remove/update/search?‚Äù
Say:
HashMap ‚Äî Best choice, O(1) avg for all operations.
ConcurrentHashMap ‚Äî If multi-threaded.
TreeMap ‚Äî If sorted order is required.

üîπ Real-world Use Cases
1. HashMap ‚Üí Fast Lookup (O(1))
üëâ Example: LRU Cache (Least Recently Used Cache)
Used in browsers, databases, OS page replacement.
We want:
Fast lookup (O(1)) ‚Üí HashMap.
Maintain usage order ‚Üí Doubly LinkedList.
Implementation:
HashMap stores <key, node> references.
Doubly LinkedList keeps track of order (move recently used to front, evict least used from back).
üìå This is exactly how LinkedHashMap in Java can be used to implement LRU Cache.

2. TreeMap ‚Üí Sorted Data
üëâ Example: Leaderboard / Ranking System (e.g., gaming platform, coding contest scoreboard)
You need:
Fast insert/update scores.
Keep scores sorted.
TreeMap stores <score, player> entries in sorted order.
Allows O(log n) search for top N scores.

3. ConcurrentHashMap ‚Üí Thread-safe Lookups
üëâ Example: Real-time Session Store (e.g., web server sessions, API rate-limiting)
Multiple threads update sessions.
HashMap is not thread-safe (may corrupt data).
ConcurrentHashMap allows concurrent reads/writes with bucket-level locks + CAS

4. ArrayList ‚Üí Random Access
üëâ Example: Playlist in a Music App
Accessing by index ‚Üí O(1)
Insertion/removal rare compared to lookups.
ArrayList is ideal here.

5. LinkedList ‚Üí Frequent Insert/Delete
üëâ Example: Undo/Redo Stack (like in editors, IDEs)
Adding/removing from head/tail ‚Üí O(1).
LinkedList works better than ArrayList if size changes frequently.

--------------------------------------------------------------------------------------

1. What data structures are used in HashMap?
Data structures used inside HashMap (Java 8+)
Array (Bucket array)
The core structure is an array of Node<K,V>[].
Each index (called a bucket) is where entries are stored after hashing the key.
The bucket index = hash(key) % capacity.
Linked List (for collisions)
When multiple keys hash to the same bucket, they are chained together as a singly linked list (Node<K,V> objects).
Before Java 8 ‚Üí always linked list.
Balanced Tree (Red-Black Tree)
Since Java 8, if too many elements end up in the same bucket (default threshold ‚â• 8 entries in a bucket), the linked list is converted into a Red-Black Tree.
Why? To reduce worst-case lookup from O(n) (linked list) ‚Üí O(log n) (tree).

Summary (Java 8+)
Array ‚Üí backbone, stores references to buckets.
Linked List ‚Üí handles collisions (when few).
Red-Black Tree ‚Üí replaces linked list when bucket grows large (‚â• 8 nodes).

HashMap (Java 8+)
 ‚îî‚îÄ‚îÄ Array of buckets
      ‚îú‚îÄ‚îÄ Bucket[0] ‚Üí Linked List (Node -> Node -> Node)
      ‚îú‚îÄ‚îÄ Bucket[1] ‚Üí Red-Black Tree (NodeA, NodeB...)
      ‚îú‚îÄ‚îÄ Bucket[2] ‚Üí null (empty)
      ‚îî‚îÄ‚îÄ ...

‚ö° In interviews, they often ask:
üëâ ‚ÄúWhy was Red-Black Tree introduced in HashMap?‚Äù
Answer:
To avoid performance degradation to O(n) in case of hash collisions (e.g., if someone attacks with many keys having same hash).
Using a balanced tree gives O(log n) lookup in such cases.

-----------------------------------------------------------------------------------------------------------------------------------------------------

- HashMap in synchronized block vs ConcurrentHashMap?
1. HashMap inside a synchronized block
Default HashMap is not thread-safe.
If multiple threads modify it at the same time ‚Üí race conditions, data corruption, or even infinite loops (before Java 8, in resizing).
To make it thread-safe, you wrap critical sections with synchronized.

Map<String, String> map = new HashMap<>();

synchronized (map) {
    map.put("A", "Apple");
    map.put("B", "Banana");
}

üîπ Characteristics
Synchronization is at the whole map level.
At any given time, only one thread can operate on the map (read/write).
This becomes a bottleneck under high concurrency.
Read operations are also blocked if a write is happening.

2. ConcurrentHashMap
Designed specifically for concurrent access.
Introduced in Java 5 (java.util.concurrent).
Internally uses:
Segmented buckets in Java 7 (locks per segment).
Bucket-level locks with CAS (compare-and-swap) in Java 8 (much finer granularity).
Allows multiple threads to read/write concurrently as long as they work on different buckets.
Example:
ConcurrentHashMap<String, String> cmap = new ConcurrentHashMap<>();
cmap.put("A", "Apple");
cmap.put("B", "Banana");
üîπ Characteristics
Better performance under concurrency
Reads are non-blocking (no lock).
Writes only block the affected bucket, not the entire map.
Null keys/values not allowed (to avoid ambiguity in concurrent environments).

| Feature                    | HashMap + synchronized block   | ConcurrentHashMap                     |
| -------------------------- | ------------------------------ | ------------------------------------- |
| Thread Safety              | Yes (if synchronized manually) | Yes (built-in)                        |
| Lock granularity           | Entire map                     | Bucket-level (finer)                  |
| Performance in concurrency | Poor (bottleneck)              | High (scales well)                    |
| Null keys/values allowed?  | Yes                            | No                                    |
| Read operations            | Blocked if synchronized        | Non-blocking                          |
| Use case                   | Low concurrency, small apps    | High concurrency, multi-threaded apps |

--------------------------------------------------------------------------------------------------------------

- Stack and queue basics, how are they implemented?
StackLL.java
Stack (LIFO ‚Äì Last In, First Out)
Concept: The last element pushed is the first one to be popped.
Operations:
push(x) ‚Üí insert at top
pop() ‚Üí remove from top
peek() ‚Üí look at top without removing
Java provides java.util.Stack class (extends Vector).
But in modern Java, ArrayDeque is preferred because it‚Äôs faster and doesn‚Äôt have legacy Vector overhead.

‚úÖ Key Difference:
Stack (legacy) ‚Üí synchronized, slower, extends Vector.
ArrayDeque (modern) ‚Üí not synchronized, faster, recommended.

Queue (FIFO ‚Äì First In, First Out)
Concept: The first element inserted is the first one to be removed.
Operations:
enqueue(x) ‚Üí add at rear
dequeue() ‚Üí remove from front
peek() ‚Üí see front


| Feature  | Stack                           | Queue                           |
| -------- | ------------------------------- | ------------------------------- |
| Order    | LIFO (Last In First Out)        | FIFO (First In First Out)       |
| Use case | Undo, function calls, recursion | Scheduling, resource sharing    |
| Impl     | Array or Linked List            | Array (circular) or Linked List |
| Main ops | push, pop, peek                 | enqueue, dequeue, peek          |


üîπ Stack (java.util.Stack)
Legacy class (since Java 1.0, extends Vector).
Thread-safe ‚Üí all methods are synchronized
Slower than ArrayDeque due to synchronization overhead.
Still works fine, but considered outdated for most use cases.
Use case:
If you really need a synchronized stack and don‚Äôt want to manage locks.
Example: legacy systems where Stack is already in use.

üîπ ArrayDeque (java.util.ArrayDeque)
Introduced in Java 1.6.
Not synchronized ‚Üí faster in single-threaded or controlled multithreaded contexts.
Implemented as a resizable circular array ‚Üí efficient push, pop, peek
Preferred for modern applications.
Use case:
If you need a stack in single-threaded or performance-critical code.
Example: parsing, DFS traversal, expression evaluation.

üîπ What if I need thread safety with ArrayDeque?
You can wrap it:
Deque<Integer> stack = Collections.synchronizedDeque(new ArrayDeque<>());


| Feature        | Stack (Legacy)                 | ArrayDeque (Modern)                                    |
| -------------- | ------------------------------ | ------------------------------------------------------ |
| Thread-safety  | Yes (synchronized methods)     | No (but can wrap with `Collections.synchronizedDeque`) |
| Performance    | Slower (due to sync)           | Faster (no sync overhead)                              |
| Implementation | Extends `Vector` (array-based) | Resizable circular array                               |
| Recommended?   | ‚ùå Avoid for new code           | ‚úÖ Preferred choice                                     |


Java has multiple built-in queue implementations in the java.util and java.util.concurrent packages.
üîπ 1. Queue Interface (java.util.Queue)
Introduced in Java 1.5
Defines common queue operations:
offer(e) ‚Üí add element (returns false if full)
poll() ‚Üí remove and return head (null if empty)
peek() ‚Üí get head without removing (null if empty)

üîπ 2. Common Queue Implementations :- QueueLL.java
‚úÖ LinkedList
Implements both Queue and Deque.
Doubly-linked list internally.
Can be used as FIFO queue or Deque (double-ended queue).

‚úÖ ArrayDeque
Resizable circular array.
Faster than LinkedList.
Implements Queue and Deque.
Cannot store null elements.

‚úÖ PriorityQueue
Elements are ordered according to natural ordering or a Comparator.
Not FIFO ‚Üí it gives you the smallest (or highest priority) element first.

üîπ 3. Concurrent Queue Implementations (java.util.concurrent)
ConcurrentLinkedQueue ‚Üí lock-free, thread-safe queue (FIFO)
LinkedBlockingQueue ‚Üí thread-safe, used in producer-consumer.
ArrayBlockingQueue ‚Üí fixed-size, bounded queue.
PriorityBlockingQueue ‚Üí thread-safe version of PriorityQueue.
DelayQueue, SynchronousQueue ‚Üí advanced specialized queues

| Queue Type            | Backed By          | Ordered? | Thread-Safe? | Use Case                    |
| --------------------- | ------------------ | -------- | ------------ | --------------------------- |
| LinkedList            | Doubly linked list | FIFO     | ‚ùå            | General purpose             |
| ArrayDeque            | Circular array     | FIFO     | ‚ùå            | Faster than LinkedList      |
| PriorityQueue         | Heap               | Priority | ‚ùå            | Scheduling, priority tasks  |
| ConcurrentLinkedQueue | Linked nodes       | FIFO     | ‚úÖ            | Non-blocking multi-threaded |
| LinkedBlockingQueue   | Linked list        | FIFO     | ‚úÖ            | Producer-Consumer           |
| ArrayBlockingQueue    | Array (fixed size) | FIFO     | ‚úÖ            | Bounded blocking queue      |

----------------------------------------------------------------------------------------------------------------


Q: ConcurrentLinkedQueue vs LinkedBlocking Queue vs ArrayBlocking Queue, and producer - consumer problem ?
üîπ 1. ConcurrentLinkedQueue
Lock-free, thread-safe implementation of a queue.
Uses CAS (Compare-And-Swap) instead of locks ‚Üí very fast in high-concurrency.
Unbounded (can grow until memory runs out).
Non-blocking ‚Üí threads don‚Äôt wait, just fail (offer() returns false if issue).

üîπ 2. LinkedBlockingQueue
Thread-safe blocking queue backed by linked nodes.
Can be bounded or unbounded (default = Integer.MAX_VALUE).
Supports blocking operations
put() ‚Üí waits if full.
take() ‚Üí waits if empty.
Used a lot in Producer-Consumer problem.
üëâ Use when: multiple producers & consumers need to coordinate with blocking.

üîπ 3. ArrayBlockingQueue
Thread-safe blocking queue backed by an array.
Fixed-size (bounded) ‚Üí capacity is set when created.
Also supports put() and take() (blocking)
Faster than LinkedBlockingQueue for predictable, bounded workloads.
üëâ Use when: you want a bounded queue with predictable performance.

-----------------------------------------------------------------------------

üîπ Producer-Consumer Problem (Classic)
Definition:
One or more Producer threads create data and put it into a queue.
One or more Consumer threads take data from the queue and process it.
Problem: we must ensure synchronization so producers don‚Äôt overwrite and consumers don‚Äôt read empty queues.
e.g.,
ProducerConsumerDemo.java

| Queue Type            | Blocking? | Bounded?  | Backed By       | Use Case                             |
| --------------------- | --------- | --------- | --------------- | ------------------------------------ |
| ConcurrentLinkedQueue | ‚ùå         | Unbounded | CAS (lock-free) | High throughput, non-blocking        |
| LinkedBlockingQueue   | ‚úÖ         | Optional  | Linked nodes    | Producer-consumer, unbounded/bounded |
| ArrayBlockingQueue    | ‚úÖ         | ‚úÖ Fixed   | Array           | Producer-consumer with fixed buffer  |

e.g., ProducerConsumerWaitNotify.java
üîπ Key Points to Notice
synchronized ‚Üí ensures only one thread modifies the buffer at a time.
wait() ‚Üí releases the lock and makes the thread wait.
notify() ‚Üí wakes up one waiting thread (either producer or consumer).
while (condition) ‚Üí always check in a loop (to avoid spurious wakeups).
This is lower-level and harder to manage ‚Üí that‚Äôs why Java introduced BlockingQueue

| Approach              | Synchronization Mechanism | Easy to Implement?                   | Recommended Today?             |
| --------------------- | ------------------------- | ------------------------------------ | ------------------------------ |
| `wait()` + `notify()` | Manual monitor locks      | ‚ùå More boilerplate, risk of deadlock | ‚ùå Only for teaching/interviews |
| `BlockingQueue`       | Built-in blocking ops     | ‚úÖ Very clean                         | ‚úÖ Yes                          |


-------------------------------------------------------------------------------


Find duplicates in an array without hashset?
‚ö° Key Idea:
We used the array itself as a "visited marker" by flipping signs. If we ever try to visit an index that's already negative ‚Üí we found a duplicate.
| Approach             | Time       | Space | Notes                          |
| -------------------- | ---------- | ----- | ------------------------------ |
| Brute Force          | O(n¬≤)      | O(1)  | Bad for large input            |
| Sort & Compare       | O(n log n) | O(1)  | Loses original order           |
| Index Marking (Best) | O(n)       | O(1)  | Works if numbers in range 1..n |

What if we need the original array later?
Option 1: Restore ‚Üí After the pass, just flip all negatives back to positive.
Option 2: Work on a copy of the array instead.

---------------------------------------------------------------------------------------

- Find duplicate strings with latest Java features.
üîπ Solution 1 ‚Äì Using Collectors.groupingBy
üîπ Solution 2 ‚Äì Using Collections.frequency (simpler, but less efficient)
üîπ Solution 3 ‚Äì Using Set trick with Streams

‚ú® Interview Tip:
Solution 1 ‚Üí Best when you want counts + duplicates (scalable).
Solution 2 ‚Üí Short but O(n¬≤) (bad for large lists).
Solution 3 ‚Üí Simple + efficient O(n).

1Ô∏è‚É£ Collections.frequency
What it does:
Counts how many times a given element appears in a collection (List, etc).
So when finding duplicates:
You can filter all elements whose Collections.frequency(list, element) > 1.

2Ô∏è‚É£ Collectors.groupingBy
What it does:
Groups elements of a stream into a Map according to a key function.

üí° Summary:
Collections.frequency ‚Üí Easy to use, good for small lists, O(n¬≤) for duplicates
Collectors.groupingBy ‚Üí Powerful for large lists + also gives counts, O(n).

frequencyMap.entrySet().stream()
Converts the Map into a stream of Map.Entry objects.
Each entry has a key (the string) and a value (the count).
Example for {"apple"=2, "banana"=1} ‚Üí entries:
Map.Entry("apple", 2)
Map.Entry("banana", 1)

filter(e -> e.getValue() > 1)
Keeps only entries where the value (count) is greater than 1.
After filtering, you get:
Map.Entry("apple", 2)

map(Map.Entry::getKey) ‚úÖ
Transforms each Map.Entry into its key only.
Why? Because we only want the duplicate strings, not their counts.
So after mapping, the stream becomes:
"apple"

collect(Collectors.toSet())
Collects all keys (strings) into a Set ‚Üí ["apple"].
Collectors.counting() is a built-in collector in Java Streams.
It counts the number of elements in a stream.
Returns a Long.